# Tiny Transformer: Implementation, Ablations & Attention Modulation

A lightweight Transformer built from scratch, with experiments on how its architecture and attention mechanisms behave.  
This repo is a playground for learning, visualizing, and tweaking Transformers.

---
## ğŸš€ Whatâ€™s Inside
- A **Tiny Transformer** implementation with configurable attention heads
- Training on a small **language translation task**
- **Ablation studies**: testing the role of residuals and feed-forward layers
- An experimental **distance-aware attention bias** that encourages local dependencies
- Visualizations of encoder self-attention, decoder self-attention, and cross-attention

---

## ğŸ“‚ Project Structure
ğŸ“‚ tiny-transformer
â”œâ”€â”€ tiny_transformer.ipynb # Notebook with implementation + experiments
â”œâ”€â”€ tiny_transformer.py # Core Python implementation
â”œâ”€â”€ report.pdf # Detailed writeup of methods and results
â”œâ”€â”€ README.md # Project docs
â””â”€â”€ requirements.txt # Dependencies

---

## âš™ï¸ Setup
You can run everything in **Google Colab** (free GPU is enough), or locally with Python 3.9+.

### 1. Clone the repo
```
git clone https://github.com/yashwantbangde/Tiny-Transformer-Implementation-Ablations-Attention-Modulation.git
cd tiny-transformer
```
### 2. Install dependencies
```
pip install -r requirements.txt
```
Or manually:
```
pip install transformers torch matplotlib seaborn pandas numpy bertviz
```
---

## ğŸš€ Experiments

### ğŸ”¹ Tiny Transformer Implementation
- Sinusoidal positional encoding
- Scaled dot-product attention
- Multi-head attention
- Feed-forward layers
- Encoder & Decoder layers
- Visualizations of encoder/decoder attentions

### ğŸ”¹ Architectural Ablations
- **Residual connections**
  - Removed
  - Weighted (learnable skip scalar)
  - Long-range skip variants
- **Feed-forward networks**
  - Bypassed to analyze role of FFN

### ğŸ”¹ Attention Modulation
- Introduced **distance-aware attention bias**:

  $$
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + B_\text{distance}\right) V
  $$

- Compared baseline vs distance-aware models
- Visualized attention distribution changes

---

## ğŸ“Š Results & Insights
- Residual connections are critical for stable training.
- FFN layers add significant non-linearity and boost performance.
- Distance-aware attention alters focus patterns, encouraging locality in token interactions.
