# Tiny Transformer: Implementation, Ablations & Attention Modulation

A lightweight Transformer built from scratch, with experiments on how its architecture and attention mechanisms behave.  
This repo is a playground for learning, visualizing, and tweaking Transformers.

---
## ðŸš€ Whatâ€™s Inside
- A **Tiny Transformer** implementation with configurable attention heads
- Training on a small **language translation task**
- **Ablation studies**: testing the role of residuals and feed-forward layers
- An experimental **distance-aware attention bias** that encourages local dependencies
- Visualizations of encoder self-attention, decoder self-attention, and cross-attention

---

## ðŸ“‚ Project Structure
ðŸ“‚ tiny-transformer
â”œâ”€â”€ tiny_transformer.ipynb # Notebook with implementation + experiments
â”œâ”€â”€ tiny_transformer.py # Core Python implementation
â”œâ”€â”€ report.pdf # Detailed writeup of methods and results
â”œâ”€â”€ README.md # Project docs
â””â”€â”€ requirements.txt # Dependencies
