# -*- coding: utf-8 -*-
"""Group_21_Assignment_1_Solutions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17YZ7MvUVWdD3-aLPgTJ0efBY87jFbC5k

# ==============================================
# ASSIGNMENT 1: DECONSTRUCTING THE TRANSFORMER
# ==============================================

# Instructions:
## 1. Use Google Colab for all experiments (free GPU tier is sufficient).
## 2. This notebook provides a complementory solution for all parts of the assignment.

# ==============================================
# SUBMISSION INSTRUCTION
# ==============================================

## 1. Please write the name of the file as `Group_(number)_assignemnt_1_solution.ipynb`

##2. Only one member from one group needs to submit the solution, to avoid any duplicasy.

# PART:1

## Tiny Transformer Implementation

You have to complete the code where it's not completed!


'''
Complete the Code
'''

## Imports and Dataset

**We are using language translation dataset for this task (WMT14 DE-EN dataset)**

- Use first 30k samples for training `[You may Increase the training Samples for better results]`

- Use first 5k samples for validation `[You may Increase the validation Samples for better generalization results]`

- Use first 1k samples for testing.

- We have provided the Helping Functions throughout the notebook, you have to complete the Code and Run as per Questions asked in Assignemnt.

- We have Alreday created the `Dataloaders` to get you started quickely and to make sure resulst can be reproduced. Please do not change the Setup and Import Section.
"""

# ============================================
# SETUP AND IMPORTS
# ============================================

# Install required packages
!pip install -q torch matplotlib seaborn numpy
!pip install -q transformers datasets tokenizers
!pip install -q sacrebleu

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset

import math
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, Optional, List, Dict

from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ============================================
# DATA LOADING AND PREPROCESSING
# ============================================

# Special tokens
PAD_TOKEN = '<pad>'
SOS_TOKEN = '<sos>'
EOS_TOKEN = '<eos>'
UNK_TOKEN = '<unk>'

# ==============================================================================
#! Change the dataloading samples as asked in the assignemnt.
# ==============================================================================

# ==============================================================================
#! Code Here
# ==============================================================================
# Load WMT14 DE-EN dataset (using a smaller subset)
print("Loading dataset...")
dataset = load_dataset("wmt14", "de-en", split={
    'train': 'train[:10000]',      # Use first ---- samples for training
    'validation': 'validation[:500]',  # Use first ---- for validation
    'test': 'test[:500]'          # Use first ---- for testing
})

print(f"Train samples: {len(dataset['train'])}")
print(f"Validation samples: {len(dataset['validation'])}")
print(f"Test samples: {len(dataset['test'])}")

# Example data point
print("\nExample data point:")
print(dataset['train'][0])

# Build tokenizers using Hugging Face tokenizers library
def build_tokenizer(texts: List[str], vocab_size: int = 10000) -> Tokenizer:
    """Build a simple word-level tokenizer"""
    tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))
    tokenizer.pre_tokenizer = Whitespace()

    trainer = WordLevelTrainer(
        vocab_size=vocab_size,
        special_tokens=[PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN],
        min_frequency=2
    )

    tokenizer.train_from_iterator(texts, trainer)
    return tokenizer

# Extract texts for tokenizer training
print("\nBuilding tokenizers...")
de_texts = [item['translation']['de'] for item in dataset['train']]
en_texts = [item['translation']['en'] for item in dataset['train']]

# Build tokenizers
de_tokenizer = build_tokenizer(de_texts, vocab_size=10000)
en_tokenizer = build_tokenizer(en_texts, vocab_size=10000)

print(f"German vocabulary size: {de_tokenizer.get_vocab_size()}")
print(f"English vocabulary size: {en_tokenizer.get_vocab_size()}")

# Get special token IDs
PAD_IDX = en_tokenizer.token_to_id(PAD_TOKEN)
SOS_IDX = en_tokenizer.token_to_id(SOS_TOKEN)
EOS_IDX = en_tokenizer.token_to_id(EOS_TOKEN)
UNK_IDX = en_tokenizer.token_to_id(UNK_TOKEN)

print(f"\nSpecial token IDs:")
print(f"PAD: {PAD_IDX}, SOS: {SOS_IDX}, EOS: {EOS_IDX}, UNK: {UNK_IDX}")

# ============================================
# DATASET CLASS AND DATA PROCESSING
# ============================================

class TranslationDataset(Dataset):
    """Custom dataset for translation"""

    def __init__(self, data, src_tokenizer, tgt_tokenizer, max_length=100):
        self.data = data
        self.src_tokenizer = src_tokenizer
        self.tgt_tokenizer = tgt_tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Get source and target sentences
        src_text = self.data[idx]['translation']['de']
        tgt_text = self.data[idx]['translation']['en']

        # Tokenize
        src_tokens = self.src_tokenizer.encode(src_text)
        tgt_tokens = self.tgt_tokenizer.encode(tgt_text)

        # Truncate if necessary
        src_tokens = src_tokens.ids[:self.max_length-2]  # Leave room for SOS/EOS
        tgt_tokens = tgt_tokens.ids[:self.max_length-2]

        # Add SOS and EOS tokens
        src_tokens = [SOS_IDX] + src_tokens + [EOS_IDX]
        tgt_tokens = [SOS_IDX] + tgt_tokens + [EOS_IDX]

        return torch.tensor(src_tokens), torch.tensor(tgt_tokens)

def collate_fn(batch):
    """Custom collate function to pad sequences"""
    src_batch, tgt_batch = [], []

    for src, tgt in batch:
        src_batch.append(src)
        tgt_batch.append(tgt)

    # Pad sequences
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)

    return src_batch, tgt_batch

# Create datasets
print("\nCreating datasets...")
train_dataset = TranslationDataset(dataset['train'], de_tokenizer, en_tokenizer)
val_dataset = TranslationDataset(dataset['validation'], de_tokenizer, en_tokenizer)
test_dataset = TranslationDataset(dataset['test'], de_tokenizer, en_tokenizer)

# Create DataLoaders
BATCH_SIZE = 64

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

# Test data loading
print("\nTesting data loading...")
src_batch, tgt_batch = next(iter(train_loader))
print(f"Source batch shape: {src_batch.shape}")
print(f"Target batch shape: {tgt_batch.shape}")

# Show example
print("\nExample tokenized pair:")
src_example = src_batch[0]
tgt_example = tgt_batch[0]

# Decode tokens back to text
src_tokens = [de_tokenizer.id_to_token(idx.item()) for idx in src_example if idx != PAD_IDX]
tgt_tokens = [en_tokenizer.id_to_token(idx.item()) for idx in tgt_example if idx != PAD_IDX]

print(f"Source tokens: {' '.join(src_tokens[:10])}...")
print(f"Target tokens: {' '.join(tgt_tokens[:10])}...")

# Vocabulary mappings for visualization
de_vocab = de_tokenizer.get_vocab()
en_vocab = en_tokenizer.get_vocab()
de_idx2word = {v: k for k, v in de_vocab.items()}
en_idx2word = {v: k for k, v in en_vocab.items()}

"""# PART:1 (A)

- Implement the following.

  - sinusoidal positional encoding

  -  Multi-head attention

  - scaled-dot product attention

  - Feed-forward layer

  - Encoder and Decoder Layer

  - Encoder and Decoder Block

## Solution 1(A)
"""

from tqdm import trange


class PositionalEncoding(nn.Module):
    """
    Add positional encoding to embeddings
    PE(pos, 2i) = sin(pos/10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
    """
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()

        # TODO: Implement positional encoding
        # Dimension of the Model
        self.d_model = d_model
        # Maximum Token Size
        self.max_len = max_len

        # Initializing the positional encoding Matrix (shape max_len, d_model)
        pe = torch.zeros(max_len, d_model)

        # Initializing a tensor to represent positions
        # Transforming position into a 2D tensor
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # Term for positional encoding formula
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # Sine for even indices in positional encoding
        pe[:, 0::2] = torch.sin(position * div_term)

        # Cosine for odd indices in positional encoding
        pe[:, 1::2] = torch.cos(position * div_term)

        # Batch Handling through an extra dimension at the begining of pe tensor
        pe = pe.unsqueeze(0)

        # Registering the positional encoding tensor as a buffer
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape (batch_size, seq_len, d_model)
        """
        # TODO: Add positional encoding to input

        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)

        return x


class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention mechanism
    """
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # TODO: Initialize linear layers for Q, K, V projections and output
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)


        self.dropout = nn.Dropout(dropout)

        # Store attention weights for visualization
        self.attention_weights = None

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            query: (batch_size, seq_len_q, d_model)
            key: (batch_size, seq_len_k, d_model)
            value: (batch_size, seq_len_v, d_model)
            mask: (batch_size, seq_len_q, seq_len_k) or None

        Returns:
            output: (batch_size, seq_len_q, d_model)
        """
        batch_size = query.size(0)

        # TODO: Implement multi-head attention
        # Project queries, keys, and values
        query = self.w_q(query)
        key = self.w_k(key)
        value = self.w_v(value)

        query = query.view(query.shape[0], query.shape[1], self.n_heads, self.d_k).transpose(1, 2)
        key = key.view(key.shape[0], key.shape[1], self.n_heads, self.d_k).transpose(1, 2)
        value = value.view(value.shape[0], value.shape[1], self.n_heads, self.d_k).transpose(1, 2)

        # Attention
        # Store weights for visualization
        output, attention_weights = self.scaled_dot_product_attention(query, key, value, mask)

        # Store the attention weights for visualization
        self.attention_weights = attention_weights.detach().cpu()

        #  Concatenate heads
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # Final linear projection
        output = self.w_o(output)

        return output

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """
        Calculate scaled dot-product attention
        """
        # TODO: Implement scaled dot-product attention
        d_k = Q.shape[-1]

        # Attention(Q,K,V) = softmax(QK^T/√d_k)V
        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

        if mask is not None:
            attention_weights = attention_weights.masked_fill(mask == 0, -1e9)

        attention_weights = attention_weights.softmax(dim=-1)
        output = torch.matmul(attention_weights, V)
        return output, attention_weights

class FeedForward(nn.Module):
    """
    Feed-forward network (FFN)
    FFN(x) = max(0, xW1 + b1)W2 + b2
    """
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Implement feed-forward network with 2 linear layrers and dropout
        # First Layer
        self.linear1 = nn.Linear(d_model, d_ff)
        # Second Layer
        self.linear2 = nn.Linear(d_ff, d_model)
        # Droupout for Overfitting prevention
        self.dropout = nn.Dropout(dropout)


    def forward(self, x: torch.Tensor) -> torch.Tensor:

        # TODO: Implement forward pass
        return self.linear2(self.dropout(torch.relu(self.linear1(x))))

class EncoderLayer(nn.Module):
    """
    Single encoder layer consisting of:
    1. Multi-head self-attention
    2. Feed-forward network
    Both with residual connections and layer normalization
    """
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Initialize components
        # Initializing the Multi-head self attention
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        # Initializing Feed-forward network
        self.feed_forward = FeedForward(d_model, d_ff, dropout)

        # Initializing the Layer Normalization
        # Initializing predefined one from Pytorch as none in mentioned in the starter code
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Initializing the dropout layers
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)


    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO: Implement encoder layer forward pass
        # Remember: residual connections and layer normalization

        # Self-attention block
        attention_output = self.self_attention(x, x, x, mask)
        # attn_weights = self.self_attention.attention_weights # Extra Code to Capture Self Attention Weights
        # Residual Connection
        x = x + self.dropout1(attention_output)
        # Normalization
        x = self.norm1(x)


        # Feed-forward block
        ff_output = self.feed_forward(x)
        # Residual Connection
        x = x + self.dropout2(ff_output)
        # Normalization
        x = self.norm2(x)

        return x # attn_weights

class DecoderLayer(nn.Module):
    """
    Single decoder layer consisting of:
    1. Masked multi-head self-attention
    2. Multi-head cross-attention
    3. Feed-forward network
    All with residual connections and layer normalization
    """
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Initialize components
        # Initializing the Masked Multi-head self attention
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)

        # Initializing the cross attention
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)

        # Initializing Feed-forward network
        self.feed_forward = FeedForward(d_model, d_ff, dropout)

        # Initializing the Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        # Initializing the dropout layers
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)


    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO: Implement decoder layer forward pass

        # Masked self-attention block
        self_attention_output = self.self_attention(x, x, x, tgt_mask)
        # self_atten_weights = self.self_attention.attention_weights # Extra Code to Capture Self Attention Weights
        # Residual Connection
        x = x + self.dropout1(self_attention_output)
        # Normalization
        x = self.norm1(x)


        # Cross-attention block
        cross_attention_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        # cross_atten_weights = self.cross_attention.attention_weights # Extra Code to Capture Cross Attention Weights
        # Residual Connection
        x = x + self.dropout2(cross_attention_output)
        # Normalization
        x = self.norm2(x)


        # Feed-forward block
        ff_output = self.feed_forward(x)
        # Residual Connection
        x = x + self.dropout3(ff_output)
        # Normalization
        x = self.norm3(x)


        return x # self_atten_weights, cross_atten_weights

class Encoder(nn.Module):
    """
    Transformer Encoder consisting of multiple encoder layers
    """
    def __init__(self, n_layers: int, d_model: int, n_heads: int,
                 d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Create stack of encoder layers with normalization
        # Stacking of Encoder Layers
        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        # Layer Normalization after all layers
        self.norm = nn.LayerNorm(d_model)


    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO: Pass input through all encoder layers
        # List to store attention weights
        # attn_weights_all = []

        # Loop to pass input to all layers
        for layer in self.layers:
            x = layer(x, mask)
            # attn_weights_all.append(attn_weights)

        # Normalizing all encoder layers
        x = self.norm(x)

        return x # attn_weights_all #TODO

class Decoder(nn.Module):
    """
    Transformer Decoder consisting of multiple decoder layers
    """
    def __init__(self, n_layers: int, d_model: int, n_heads: int,
                 d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Create stack of decoder layers with niormlaization
        # Stack of Decoder Layer
        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        # Final layer normalization
        self.norm = nn.LayerNorm(d_model)


    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:

        # TODO: Pass input through all decoder layers
        # List to capture self attention weights
        # self_attn_all = []
        # List to capture cross attention weights
        # cross_attn_all = []
        # for loop to pass input through all layers
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
            # self_attn_all.append(self_attn_weights)
            # cross_attn_all.append(cross_attn_weights)

        # Normalizing all decoder layers
        x = self.norm(x)


        return x # self_attn_all, cross_attn_all #TODO

"""## PART - 1(B)

Implement a Tiny Transformer with the following specifications:

    - Embedding dimension: 128

    - Transformer Layers 2

    - Configurable number of attention heads (1,2,4..8 etc)

    - Feed-Forward dim: 512

    - Max-Token Seq Length: 128

## Solution 1(B)
"""

class TinyTransformer(nn.Module):
    """
    Complete Transformer model for translation

    Architecture:
    - Source embedding + positional encoding
    - Target embedding + positional encoding
    - Encoder stack
    - Decoder stack
    - Output projection layer
    """
    def __init__(self, src_vocab_size: int, tgt_vocab_size: int,
                 d_model: int = 256, n_heads: int = 4, n_layers: int = 2,
                 d_ff: int = 1024, max_seq_len: int = 100, dropout: float = 0.1):
        super().__init__()

        # Model parameters
        self.d_model = d_model
        self.n_heads = n_heads

        # TODO: Initialize all components
        # Embeddings
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)

        # Positional encoding
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)


        # Encoder and Decoder stacks
        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)
        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)


        # Output projection
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)


        # Dropout
        self.dropout = nn.Dropout(dropout)


        # Initialize parameters
        self._init_parameters()

    def _init_parameters(self):
        """Initialize parameters with Xavier uniform"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def create_src_mask(self, src: torch.Tensor) -> torch.Tensor:
        """Create source mask to hide padding tokens"""

        # TODO: Create mask where True indicates valid position
        # Id of PAD token is 0
        pad_idx = 0

        src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)

        return src_mask#TODO

    def create_tgt_mask(self, tgt: torch.Tensor) -> torch.Tensor:
        """Create target mask to hide padding tokens and future positions"""
        # TODO: Create mask combining padding mask and causal mask


        # Padding mask
        pad_idx = 0
        batch_size, tgt_len = tgt.size()
        padding_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)

        # Causal mask (lower triangular)
        causal_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()

        # Combine masks
        tgt_mask = padding_mask & causal_mask.unsqueeze(0).unsqueeze(1)


        return tgt_mask #TODO

    def encode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:
        """Encode source sequence"""
        # TODO: Implement encoding
        # 1. Embed source tokens
        src_embedded = self.src_embedding(src)

        # 2. Scale embeddings by sqrt(d_model)
        src_embedded = src_embedded * math.sqrt(self.d_model)

        # 3. Add positional encoding
        src_embedded = self.pos_encoding(src_embedded)

        # 4. Apply dropout
        src_embedded = self.dropout(src_embedded)

        # 5. Pass through encoder
        # Addition of enc_attn
        encoder_output = self.encoder(src_embedded, src_mask)

        # Update enc_attn in return also
        return encoder_output #TODO

    def decode(self, tgt: torch.Tensor, encoder_output: torch.Tensor,
               src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:
        """Decode target sequence"""
        # TODO: Implement decoding
        # Similar to encoding but with decoder

        # 1. Embed source tokens
        tgt_embedded = self.tgt_embedding(tgt)

        # 2. Scale embeddings by sqrt(d_model)
        tgt_embedded = tgt_embedded * math.sqrt(self.d_model)

        # 3. Add positional encoding
        tgt_embedded = self.pos_encoding(tgt_embedded)

        # 4. Apply dropout
        tgt_embedded = self.dropout(tgt_embedded)

        # 5. Pass through encoder
        # Update dec_self_attn, dec_cross_attn
        decoder_output = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)

        # Also updated in return dec_self_attn, dec_cross_attn
        return decoder_output #TODO

    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for training

        Args:
            src: Source sequences (batch_size, src_seq_len)
            tgt: Target sequences (batch_size, tgt_seq_len)

        Returns:
            output: Predicted token logits (batch_size, tgt_seq_len, tgt_vocab_size)
        """
        # TODO: Implement complete forward pass
        # 1. Create masks
        src_mask = self.create_src_mask(src)
        tgt_mask = self.create_tgt_mask(tgt)

        # Updated in Encode and Decode
        # Update enc_attn in encode
        # 2. Encode source
        encoder_output = self.encode(src, src_mask)

        # updated decode with dec_self_attn, dec_cross_attn
        # 3. Decode target
        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)

        # Storing Weights for Later Use
        # self.attention_weights = {
        #     "encoder_self_attention": enc_attn,
        #     "decoder_self_attention": dec_self_attn,
        #     "decoder_cross_attention": dec_cross_attn,
        # }

        # 4. Project to vocabulary
        output = self.output_projection(decoder_output)


        return output #TODO

    def get_attention_weights(self):
        """
        Extract attention weights from all layers for visualization

        Returns dict with:
        - encoder_self_attention: List of attention weights from encoder layers
        - decoder_self_attention: List of attention weights from decoder self-attention
        - decoder_cross_attention: List of attention weights from decoder cross-attention
        """
        attention_weights = {
            'encoder_self_attention': [],
            'decoder_self_attention': [],
            'decoder_cross_attention': []
        }

        # TODO: Extract attention weights from all layers
        # Encoder self-attention
        for layer in self.encoder.layers:
          if hasattr(layer.self_attention, 'attention_weights'):
            attention_weights['encoder_self_attention'].append(layer.self_attention.attention_weights)


        # Decoder self-attention and cross-attention
        for layer in self.decoder.layers:
          if hasattr(layer.self_attention, 'attention_weights'):
            attention_weights['decoder_self_attention'].append(layer.self_attention.attention_weights)
          if hasattr(layer.cross_attention, 'attention_weights'):
            attention_weights['decoder_cross_attention'].append(layer.cross_attention.attention_weights)


        return attention_weights #TODO

"""## PART 1(C-D)

- Train multi head model (4 heads) and single head (1) model, by keeping the number of parameters same, adjust attention head dimension accordingly.

- Implement visualization for different attention types like Encoder self-attention, Decoder self-attention and Decoder cross-attention. Visualize the attentions for multi-head and single-head both for given test sentences.

## Solution 1(C-D)
"""

from tqdm import tqdm, trange

# ============================================
# TODO: WRITE TRAINING FUNCTIONS
# ============================================

def train_epoch(model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer,
                criterion: nn.Module, clip: float = 1.0) -> float:
    """Train for one epoch"""
    model.train()

    # TODO
    epoch_loss = 0

    # iterate through batches in the dataloader
    for src, tgt in dataloader:
      src, tgt = src.to(device), tgt.to(device)

      # As a translation task we split the data into input and output
      tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]

      optimizer.zero_grad()

      # Forward pass
      output = model(src, tgt_input)

      # Reshaping the loss calculation
      output = output.reshape(-1, output.size(-1))
      tgt_output = tgt_output.reshape(-1)

      # Loss Calculation
      loss = criterion(output, tgt_output)

      # Backward Pass
      loss.backward()

      # Gradient clipping
      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

      # Optimizer step
      optimizer.step()

      epoch_loss = epoch_loss + loss.item()

    return epoch_loss / len(dataloader) # epoch_loss / len(dataloader)

def evaluate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module) -> float:
    """Evaluate model"""
    model.eval()
    epoch_loss = 0

    # TODO
    # As evaluation no gradient descent calculation
    with torch.no_grad():
      # Loop through in batches over the dataloader
      for src, tgt in dataloader:
        src, tgt = src.to(device), tgt.to(device)

        # Input Output Preparation
        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]

        # Forward pass
        output = model(src, tgt_input)

        # Reshaping the loss calculation
        output = output.reshape(-1, output.size(-1))
        tgt_output = tgt_output.reshape(-1)

        # Loss Calculation
        loss = criterion(output, tgt_output)

        epoch_loss = epoch_loss + loss.item()

    return epoch_loss / len(dataloader) # epoch_loss / len(dataloader)

# ============================================
# TODO: WRITE TRANSLATING SENTENCE FUNCTION
# ============================================

# Need to declare special tokens for both english and german texts
# For German (source)
SRC_SOS = de_tokenizer.token_to_id(SOS_TOKEN)
SRC_EOS = de_tokenizer.token_to_id(EOS_TOKEN)

# For English (target)
TGT_SOS = en_tokenizer.token_to_id(SOS_TOKEN)
TGT_EOS = en_tokenizer.token_to_id(EOS_TOKEN)


def translate_sentence(model: nn.Module, src_sentence: str, max_length: int = 50):
    """Translate a single sentence"""
    model.eval()

    # Tokenize source sentence
    src_tokens = [SRC_SOS] + de_tokenizer.encode(src_sentence).ids + [SRC_EOS]
    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device)

    # Encode the input
    with torch.no_grad():
      encoder_output = model.encode(src_tensor, src_mask=None)

    tgt_indices = [TGT_SOS]

    # Autoregressive decoding
    for _ in range(max_length):
      tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)

      # Decode the output
      with torch.no_grad():
        output = model.decode(tgt_tensor, encoder_output, src_mask=None, tgt_mask=None)
        output = model.output_projection(output)

      next_token = output[:, -1, :].argmax(dim=-1).item()
      tgt_indices.append(next_token)

      if next_token == TGT_EOS:
        break

      # Indices translated back to words
      translation = [en_idx2word.get(idx, '<unk>') for idx in tgt_indices[1:-1]]

    return " ".join(translation), tgt_indices # translation, tgt_indices

# ============================================
# TODO: WRITE ATTENTION VISUALIZATION FUNCTION
# ============================================


# ========================================================================
# TODO: YOU CAN USE THE ATTENTION VISUALIZATION TOOLS : ADD THE PLOTS HERE
# ========================================================================

def visualize_attention(model: nn.Module, src_sentence: str, tgt_sentence: str = None):
    """
    Visualize attention weights for all heads in all layers
    """
    model.eval()



    # TODO: Implement visualization for different attention types
    translation, tgt_indices = translate_sentence(model, src_sentence)

    if tgt_sentence is None:
      tgt_tokens = ["<sos>"] + translation.split() + ["<eos>"]
    else:
      tgt_tokens = ["<sos>"] + tgt_sentence.split() + ["<eos>"]

    # Source Tokens
    src_tokens = ["<sos>"] + src_sentence.split() + ["<eos>"]

    # Stored attention weights fetching
    attention_weights = model.get_attention_weights()

    # Check if actually we got weights
    if not any(len(v) > 0 for v in attention_weights.values()):
        print("No attention weights were recorded")
        return

    # Plotting Attention Weights
    # 1. Encoder self-attention
    for layer, layer_attn in enumerate(attention_weights['encoder_self_attention']):

      # Taking only the batch for visualization
      if layer_attn.dim() == 4:
        layer_attn = layer_attn[0]

      for head in range(layer_attn.shape[0]):
        plt.figure(figsize=(8, 6))
        sns.heatmap(layer_attn[head].detach().cpu().numpy(), xticklabels=src_tokens, yticklabels=src_tokens, cmap="viridis")
        plt.title(f"Encoder Self-Attention (Layer {layer+1}, Head {head+1})")
        plt.xlabel("Key (Source)")
        plt.ylabel("Query (Source)")
        plt.show()

    # 2. Decoder self-attention
    for layer, layer_attn in enumerate(attention_weights['decoder_self_attention']):

      # Taking only the batch for visualization
      if layer_attn.dim() == 4:
        layer_attn = layer_attn[0]

      for head in range(layer_attn.shape[0]):
          plt.figure(figsize=(7, 6))
          sns.heatmap(layer_attn[head].detach().cpu().numpy(), xticklabels=tgt_tokens, yticklabels=tgt_tokens, cmap="magma")
          plt.title(f"Decoder Self-Attention (Layer {layer+1}, Head {head+1})")
          plt.xlabel("Key (Target)")
          plt.ylabel("Query (Target)")
          plt.show()

    # 3. Decoder cross-attention
    for layer, layer_attn in enumerate(attention_weights['decoder_cross_attention']):

      # Taking only the batch for visualization
      if layer_attn.dim() == 4:
        layer_attn = layer_attn[0]

      for head in range(layer_attn.shape[0]):
          plt.figure(figsize=(8, 6))
          sns.heatmap(layer_attn[head].detach().cpu().numpy(), xticklabels=src_tokens, yticklabels=tgt_tokens, cmap="plasma")
          plt.title(f"Decoder Cross-Attention (Layer {layer+1}, Head {head+1})")
          plt.xlabel("Source")
          plt.ylabel("Target")
          plt.show()


# ===================================================
# TODO: WRITE ATTENTION COMPARISON PATTERNS FUNCTION
# ===================================================

def compare_attention_patterns(model_multi: nn.Module, model_single: nn.Module,
                             src_sentence: str):
    """
    Compare attention patterns between multi-head and single-head models
    """
    print("\n=== COMPARING MULTI-HEAD vs SINGLE-HEAD ATTENTION ===")

  # TODO
    translation_multi, tgt_multi_indices = translate_sentence(model_multi, src_sentence)
    translation_single, tgt_single_indices = translate_sentence(model_single, src_sentence)

    src_tokens = ["<sos>"] + src_sentence.split() + ["<eos>"]
    tgt_multi_tokens = ["<sos>"] + translation_multi.split() + ["<eos>"]
    tgt_single_tokens = ["<sos>"] + translation_single.split() + ["<eos>"]

    print(f"Source Sentence: {src_sentence}")
    print(f"Multi-Head Translation: {translation_multi}")
    print(f"Single-Head Translation: {translation_single}")

    # Storing attention weights
    attention_weights_multi = model_multi.get_attention_weights()
    attention_weights_single = model_single.get_attention_weights()

    enc_multi = attention_weights_multi['encoder_self_attention'][0]
    enc_single = attention_weights_single['encoder_self_attention'][0]

    # Remove batch dim if it exists to make the dimension compatible
    if enc_multi.dim() == 4:
        enc_multi = enc_multi[0]
    if enc_single.dim() == 4:
        enc_single = enc_single[0]


    # Encoder Self Attention:
    for head in range(enc_multi.shape[0]):
      plt.figure(figsize=(8, 6))

      # Multi-head:
      plt.subplot(1, 2, 1)
      sns.heatmap(enc_multi[head].detach().cpu().numpy(), xticklabels=src_tokens, yticklabels=src_tokens, cmap="viridis")
      plt.title(f"Multi-Head (Head {head+1}) Encoder Self-Attention")

      # Single-head:
      plt.subplot(1, 2, 2)
      sns.heatmap(enc_single[0].detach().cpu().numpy(), xticklabels=src_tokens, yticklabels=src_tokens, cmap="viridis")
      plt.title(f"Single-Head Encoder Self-Attention")

      plt.show()

    # Decoder Cross-Attention
    cross_attention_multi = attention_weights_multi['decoder_cross_attention'][0]
    cross_attention_single = attention_weights_single['decoder_cross_attention'][0]

    # Remove batch dim if it exists to make the dimension compatible
    if cross_attention_multi.dim() == 4:
        cross_attention_multi = cross_attention_multi[0]
    if cross_attention_single.dim() == 4:
        cross_attention_single = cross_attention_single[0]

    for head in range(cross_attention_multi.shape[0]):
      plt.figure(figsize=(8, 6))

      # Multi-head:
      plt.subplot(1, 2, 1)
      sns.heatmap(cross_attention_multi[head].detach().cpu().numpy(), xticklabels=src_tokens, yticklabels=tgt_multi_tokens, cmap="plasma")
      plt.title(f"Multi-Head (Head {head+1}) Decoder Cross-Attention")

      # Single-head
      plt.subplot(1, 2, 2)
      sns.heatmap(cross_attention_single[0].detach().cpu().numpy(), xticklabels=src_tokens, yticklabels=tgt_single_tokens, cmap="plasma")
      plt.title(f"Single-Head Decoder Cross-Attention")

      plt.show()

    print("Comparison Plots Complete")





# ============================================
# TODO: WRITE MAIN TRAINING SCRIPT
# ============================================

def count_parameters(model):
    """Count trainable parameters"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad) # TODO

# Model configurations
MODEL_CONFIGS = {
    'multi_head': {
        'd_model': 128,
        'n_heads': 4,
        'n_layers': 2,
        'd_ff': 256,
        'dropout': 0.1
    },
    'single_head': {
        'd_model': 128,
        'n_heads': 1,
        'n_layers': 2,
        'd_ff': 456,
        'dropout': 0.1
    }
}

# Initialize models
print("Initializing models...")

model_multi = TinyTransformer(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['multi_head']
).to(device)

model_single = TinyTransformer(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['single_head']
).to(device)

print(f"Multi-head model parameters: {count_parameters(model_multi):,}")
print(f"Single-head model parameters: {count_parameters(model_single):,}")

# Training settings
LEARNING_RATE = 0.0001 # YOU CAN CHOOSE AS PER TRAINING AND VALIDATION PLOTS
N_EPOCHS = 100 # TODO

# Loss function - ignore padding token
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# ============================================
# TRAINING LOOP FOR MULTI-HEAD MODEL
# ============================================

print("\n" + "="*50)
print("TRAINING MULTI-HEAD MODEL (4 heads)")
print("="*50)

optimizer_multi = optim.Adam(model_multi.parameters(), lr=LEARNING_RATE)

# Training history
train_losses_multi = []
val_losses_multi = []

best_val_loss = float('inf')

for epoch in trange(N_EPOCHS, desc="Epochs"):
    start_time = time.time()

    # Train
    train_loss = train_epoch(model_multi, train_loader, optimizer_multi, criterion)

    # Evaluate
    val_loss = evaluate(model_multi, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_multi.append(train_loss)
    val_losses_multi.append(val_loss)

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_multi.state_dict(), 'tiny_transformer_multi.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
model_multi.load_state_dict(torch.load('tiny_transformer_multi.pt'))

# ============================================
# TODO: TRAINING LOOP FOR SINGLE-HEAD MODEL
# ============================================

print("\n" + "="*50)
print("TRAINING SINGLE-HEAD MODEL")
print("="*50)

optimizer_single = optim.Adam(model_single.parameters(), lr=LEARNING_RATE)

# Training history
train_losses_single = []
val_losses_single = []

best_val_loss = float('inf')  # Same as Multi Head # TODO

for epoch in trange(N_EPOCHS):
    start_time = time.time()

    # Using same code as above just changing the input to single model and changing some variable name to single
    # Train
    train_loss = train_epoch(model_single, train_loader, optimizer_single, criterion)


    # Evaluate
    val_loss = evaluate(model_single, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_single.append(train_loss)
    val_losses_single.append(val_loss)



    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_single.state_dict(), 'tiny_transformer_single.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
# TODO
model_single.load_state_dict(torch.load('tiny_transformer_single.pt'))


# ============================================
# TODO: PLOT TRAINING CURVES
# ============================================

plt.figure(figsize=(12, 5))

# Loss curves
plt.subplot(1, 2, 1)
plt.plot(train_losses_multi, label='Multi-head Train')
plt.plot(val_losses_multi, label='Multi-head Val')
plt.plot(train_losses_single, label='Single-head Train')
plt.plot(val_losses_single, label='Single-head Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# Validation loss comparison
plt.subplot(1, 2, 2)
epochs = range(1, N_EPOCHS + 1)
plt.plot(epochs, val_losses_multi, 'o-', label='Multi-head')
plt.plot(epochs, val_losses_single, 's-', label='Single-head')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss Comparison')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# ============================================
# TODO: EVALUATION AND VISUALIZATION
# ============================================

print("\n" + "="*50)
print("EVALUATION ON TEST SENTENCES")
print("="*50)

# Test sentences
test_sentences = [
    "Ein Mann läuft auf der Straße.",  # A man walks on the street
    "Die Katze sitzt auf dem Tisch.",   # The cat sits on the table
    "Ich liebe dich.",                  # I love you
    "Das Wetter ist heute schön.",      # The weather is nice today
    "Können Sie mir helfen?"            # Can you help me?
]

# Translate with both models
print("\nTranslation Examples:")
for i, src in enumerate(test_sentences):
    print(f"\n{i+1}. Source: {src}")

    trans_multi, _ = translate_sentence(model_multi, src)
    print(f"   Multi-head: {trans_multi}")

    trans_single, _ = translate_sentence(model_single, src)
    print(f"   Single-head: {trans_single}")

# ============================================
# ATTENTION VISUALIZATION FOR TEST SENTENCE
# ============================================

print("\n" + "="*50)
print("ATTENTION VISUALIZATION")
print("="*50)

# Choose a test sentence for detailed visualization
test_sentence = "Die Katze sitzt auf dem Tisch."
print(f"\nVisualizing attention for: '{test_sentence}'")

# Visualize multi-head model attention
print("\n### MULTI-HEAD MODEL ###")
visualize_attention(model_multi, test_sentence)

# Visualize single-head model attention
print("\n### SINGLE-HEAD MODEL ###")
visualize_attention(model_single, test_sentence)

# Compare attention patterns
compare_attention_patterns(model_multi, model_single, test_sentence)

"""# PART: 2 - Architecture Ablation Study

## PART 2(A)

**Study the Role of Residual Connections**

## Solution 2(A)
"""

# =========================================================================================================================
# TODO: ARCHITECTURAL ABLATION STUDIES [YOU MAY CHOOSE DIFFERENT IMPLEMENTATION STRATEGY AS LONG AS YOU IMPLEMENT AS ASKED]
# =========================================================================================================================

# --- Experiment 1: Removing Residual Connections ---

class EncoderLayerNoResidual(EncoderLayer):
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:

       # TODO
       # Self-attention block without residual connection
        attention_output = self.self_attention(x, x, x, mask)
        x = self.dropout1(attention_output)
        x = self.norm1(x)

        # Feed-forward block without residual connection
        ff_output = self.feed_forward(x)
        x = self.dropout2(ff_output)
        x = self.norm2(x)
        return x # TODO

class DecoderLayerNoResidual(DecoderLayer):
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO
        # Masked self-attention block
        self_attention_output = self.self_attention(x, x, x, tgt_mask)
        # self_atten_weights = self.self_attention.attention_weights # Extra Code to Capture Self Attention Weights
        # No Residual Connection
        x = self.dropout1(self_attention_output)
        # Normalization
        x = self.norm1(x)


        # Cross-attention block
        cross_attention_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        # cross_atten_weights = self.cross_attention.attention_weights # Extra Code to Capture Cross Attention Weights
        # No Residual Connection
        x = self.dropout2(cross_attention_output)
        # Normalization
        x = self.norm2(x)


        # Feed-forward block
        ff_output = self.feed_forward(x)
        # No Residual Connection
        x = self.dropout3(ff_output)
        # Normalization
        x = self.norm3(x)
        return x # TODO

class EncoderNoResidual(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # TODO
        # Override with residual-free ones
        self.layers = nn.ModuleList([EncoderLayerNoResidual(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])

class DecoderNoResidual(Decoder):
     def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # TODO
        # Override with residual-free ones
        self.layers = nn.ModuleList([DecoderLayerNoResidual(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])

class TransformerNoResidual(TinyTransformer):
    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 256, n_heads: int = 4, n_layers: int = 2, d_ff: int = 1024, max_seq_len: int = 100, dropout: float = 0.1):
        super().__init__(src_vocab_size, tgt_vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout)

        # TODO
        self.encoder = EncoderNoResidual(n_layers, d_model, n_heads, d_ff, dropout)
        self.decoder = DecoderNoResidual(n_layers, d_model, n_heads, d_ff, dropout)

model_no_residual = TransformerNoResidual(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['multi_head']
).to(device) # TODO: INITIALIZE THE MODEL

# TODO -- RUN TRAINING
print(f"Multi-head No Residual model parameters: {count_parameters(model_no_residual):,}")


# Training settings
LEARNING_RATE = 0.0001 # YOU CAN CHOOSE AS PER TRAINING AND VALIDATION PLOTS
N_EPOCHS = 100 # TODO

# Loss function - ignore padding token
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# ============================================
# TRAINING LOOP FOR MULTI-HEAD MODEL
# ============================================

print("\n" + "="*50)
print("TRAINING MULTI-HEAD MODEL (4 heads)")
print("="*50)

optimizer_multi = optim.Adam(model_multi.parameters(), lr=LEARNING_RATE)

# Training history
train_losses_multi_no_residual = []
val_losses_multi_no_residual = []

best_val_loss = float('inf')

for epoch in trange(N_EPOCHS, desc="Epochs"):
    start_time = time.time()

    # Train
    train_loss = train_epoch(model_no_residual, train_loader, optimizer_multi, criterion)

    # Evaluate
    val_loss = evaluate(model_no_residual, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_multi_no_residual.append(train_loss)
    val_losses_multi_no_residual.append(val_loss)

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_no_residual.state_dict(), 'tiny_transformer_no_residual_multi.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
model_multi.load_state_dict(torch.load('tiny_transformer_no_residual_multi.pt'))

# ============================================
# TODO: PLOT TRAINING CURVES
# ============================================

plt.figure(figsize=(12, 5))

# Loss curves
plt.subplot(1, 2, 1)
plt.plot(train_losses_multi_no_residual, label='Multi-head Train')
plt.plot(val_losses_multi_no_residual, label='Multi-head Val')
# plt.plot(train_losses_single, label='Single-head Train')
# plt.plot(val_losses_single, label='Single-head Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# Validation loss comparison
plt.subplot(1, 2, 2)
epochs = range(1, N_EPOCHS + 1)
plt.plot(epochs, val_losses_multi, 'o-', label='Multi-head')
plt.plot(epochs, val_losses_multi_no_residual, 's-', label='Multi-head-No-Residual')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
# plt.title('Validation Loss Comparison')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""## IMPLEMNT **Learnable skip weights** AND TRAIN AGAIN TO OBSERVE THE CHANGES."""

# --- Experiment 2: IMPLEMENT LEARNABLE SKIP WEIGHTS ---


# TODO: WRITE THE CODE SIMILIARLY

class EncoderLayerLearnableSkipWeights(EncoderLayer):

    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(d_model, n_heads, d_ff, dropout)

        # Addition of an extra Weight Bias Matrix W for both Self Attention and Feed Forward
        self.w1 = nn.Parameter(torch.tensor(1.0))
        self.w2 = nn.Parameter(torch.tensor(1.0))

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:

       # TODO
       # Self-attention block with Learnable Skip Weights
        attention_output = self.self_attention(x, x, x, mask)
        x = self.w1 * x + self.dropout1(attention_output)
        x = self.norm1(x)

        # Feed-forward block without residual connection
        ff_output = self.feed_forward(x)
        x = self.w2 * x + self.dropout2(ff_output)
        x = self.norm2(x)
        return x # TODO

class DecoderLayerLearnableSkipWeights(DecoderLayer):

    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(d_model, n_heads, d_ff, dropout)

        # # Addition of an extra Weight Bias Matrix W for Self Attention, Cross Attention and Feed-Forward
        self.w1 = nn.Parameter(torch.tensor(1.0))
        self.w2 = nn.Parameter(torch.tensor(1.0))
        self.w3 = nn.Parameter(torch.tensor(1.0))

    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO
        # Masked self-attention block
        self_attention_output = self.self_attention(x, x, x, tgt_mask)
        # Self-attention block with Learnable Skip Weights
        x = self.w1 * x + self.dropout1(self_attention_output)
        # Normalization
        x = self.norm1(x)


        # Cross-attention block
        cross_attention_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        # With Learnable Skip Weights
        x = self.w2 * x + self.dropout2(cross_attention_output)
        # Normalization
        x = self.norm2(x)


        # Feed-forward block
        ff_output = self.feed_forward(x)
        # With Learnable Skip Weights
        x = self.w3 * x + self.dropout3(ff_output)
        # Normalization
        x = self.norm3(x)
        return x # TODO

class EncoderLearnableSkipWeights(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # TODO
        # Override with Learnable Skip Weights ones
        self.layers = nn.ModuleList([ EncoderLayerLearnableSkipWeights(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        # Final Normalization
        self.norm = nn.LayerNorm(d_model)

class DecoderLearnableSkipWeights(Decoder):
     def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # TODO
        # Override with Learnable Skip Weights
        self.layers = nn.ModuleList([ DecoderLayerLearnableSkipWeights(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        # Normalization
        self.norm = nn.LayerNorm(d_model)

class TransformerLearnableSkipWeights(TinyTransformer):
    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 256, n_heads: int = 4, n_layers: int = 2, d_ff: int = 1024, max_seq_len: int = 100, dropout: float = 0.1):
        super().__init__(src_vocab_size, tgt_vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout)
        # TODO
        self.encoder = EncoderLearnableSkipWeights(n_layers, d_model, n_heads, d_ff, dropout)
        self.decoder = DecoderLearnableSkipWeights(n_layers, d_model, n_heads, d_ff, dropout)

model_learnable_skip_weights = TransformerNoResidual(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['multi_head']
).to(device) # TODO: INITIALIZE THE MODEL

# TODO -- RUN TRAINING
print(f"Multi-head Learnable Skip Weights model parameters: {count_parameters(model_learnable_skip_weights):,}")


# Training settings
LEARNING_RATE = 0.0001 # YOU CAN CHOOSE AS PER TRAINING AND VALIDATION PLOTS
N_EPOCHS = 100 # TODO

# Loss function - ignore padding token
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# ============================================
# TRAINING LOOP FOR MULTI-HEAD MODEL
# ============================================

print("\n" + "="*50)
print("TRAINING MULTI-HEAD MODEL (4 heads)")
print("="*50)

optimizer_multi = optim.Adam(model_learnable_skip_weights.parameters(), lr=LEARNING_RATE)

# Training history
train_losses_multi_learnable_skip_weights = []
val_losses_multi_learnable_skip_weights = []

best_val_loss = float('inf')

for epoch in trange(N_EPOCHS, desc="Epochs"):
    start_time = time.time()

    # Train
    train_loss = train_epoch(model_learnable_skip_weights, train_loader, optimizer_multi, criterion)

    # Evaluate
    val_loss = evaluate(model_learnable_skip_weights, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_multi_learnable_skip_weights.append(train_loss)
    val_losses_multi_learnable_skip_weights.append(val_loss)

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_no_residual.state_dict(), 'tiny_transformer_learnable_skip_weights.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
model_multi.load_state_dict(torch.load('tiny_transformer_learnable_skip_weights.pt'))

# ============================================
# TODO: PLOT TRAINING CURVES
# ============================================

plt.figure(figsize=(12, 5))

# Loss curves
plt.subplot(1, 2, 1)
plt.plot(train_losses_multi_learnable_skip_weights, label='Multi-head Train')
plt.plot(val_losses_multi_learnable_skip_weights, label='Multi-head Val')
# plt.plot(train_losses_single, label='Single-head Train')
# plt.plot(val_losses_single, label='Single-head Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# Validation loss comparison
plt.subplot(1, 2, 2)
epochs = range(1, N_EPOCHS + 1)
plt.plot(epochs, val_losses_multi, 'o-', label='Multi-head')
plt.plot(epochs, val_losses_multi_learnable_skip_weights, 's-', label='Multi-head-Learnable-Skip-Weights')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss Comparison')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""## IMPLEMNT **Long-Range Skip Connections** AND TRAIN AGAIN TO OBSERVE THE CHANGES."""

# --- Experiment 3: IMPLEMENT LONG RANGE SKIP CONNECTION ---


# TODO: WRITE THE CODE SIMILIARLY

# To implement Long Range Skip Connection we can only modify the Encoder, Decoder and TinyTransormer class by inheriting from them

class EncoderLongRange(Encoder):
  # Just changing the forward function should be enough
  # Implementing such that next layer output is calculated on current and previous layes
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        previous_layer_x = x

        for layer in self.layers:
            # Swaping the Layers inside the for loop should be enough by explicitly holding the layer values
            current_layer_x = layer(x, mask)
            x = current_layer_x + previous_layer_x
            previous_layer_x = current_layer_x

        x = self.norm(x)
        return x

# Doing the Exact Same thing but for Decoder
class DecoderLongRange(Decoder):
      def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:

        prev_x = x

        for layer in self.layers:
            current_x = layer(x, encoder_output, src_mask, tgt_mask)
            x = current_x + prev_x
            prev_x = current_x

        x = self.norm(x)
        return x

class TransformerLongRange(TinyTransformer):
    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 256, n_heads: int = 4, n_layers: int = 2, d_ff: int = 1024, max_seq_len: int = 100, dropout: float = 0.1):
        super().__init__(src_vocab_size, tgt_vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout)
        # TODO
        self.encoder = EncoderLongRange(n_layers, d_model, n_heads, d_ff, dropout)
        self.decoder = DecoderLongRange(n_layers, d_model, n_heads, d_ff, dropout)

model_learnable_long_range = TransformerLongRange(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['multi_head']
).to(device) # TODO: INITIALIZE THE MODEL

# TODO -- RUN TRAINING
print(f"Multi-head Long Range Skip Connections model parameters: {count_parameters(model_learnable_long_range):,}")


# Training settings
LEARNING_RATE = 0.0001 # YOU CAN CHOOSE AS PER TRAINING AND VALIDATION PLOTS
N_EPOCHS = 100 # TODO

# Loss function - ignore padding token
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# ============================================
# TRAINING LOOP FOR MULTI-HEAD MODEL
# ============================================

print("\n" + "="*50)
print("TRAINING MULTI-HEAD MODEL (4 heads)")
print("="*50)

optimizer_multi = optim.Adam(model_learnable_long_range.parameters(), lr=LEARNING_RATE)

# Training history
train_losses_multi_learnable_long_range = []
val_losses_multi_learnable_long_range = []

best_val_loss = float('inf')

for epoch in trange(N_EPOCHS, desc="Epochs"):
    start_time = time.time()

    # Train
    train_loss = train_epoch(model_learnable_long_range, train_loader, optimizer_multi, criterion)

    # Evaluate
    val_loss = evaluate(model_learnable_long_range, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_multi_learnable_long_range.append(train_loss)
    val_losses_multi_learnable_long_range.append(val_loss)

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_no_residual.state_dict(), 'tiny_transformer_learnable_long_range.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
model_multi.load_state_dict(torch.load('tiny_transformer_learnable_long_range.pt'))

# ============================================
# TODO: PLOT TRAINING CURVES
# ============================================

plt.figure(figsize=(12, 5))

# Loss curves
plt.subplot(1, 2, 1)
plt.plot(train_losses_multi_learnable_long_range, label='Multi-head Train')
plt.plot(val_losses_multi_learnable_long_range, label='Multi-head Val')
# plt.plot(train_losses_single, label='Single-head Train')
# plt.plot(val_losses_single, label='Single-head Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# Validation loss comparison
plt.subplot(1, 2, 2)
epochs = range(1, N_EPOCHS + 1)
plt.plot(epochs, val_losses_multi, 'o-', label='Multi-head')
plt.plot(epochs, val_losses_multi_learnable_long_range, 's-', label='Multi-head-Learnable-Long-Range')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
# plt.title('Validation Loss Comparison')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""## PART 2(B)

**Study the Role of Feed-Forward Layers**

- Answer the questions as asked in the Assignemnt.

## Solution 2(B)
"""

# --- Experiment 1: Removing Feed-Forward Layers ---

class EncoderLayerNoFFN(EncoderLayer):
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self-attention block
        attention_output = self.self_attention(x, x, x, mask)
        # Residual connection + LayerNorm
        x = x + self.dropout1(attention_output)
        x = self.norm1(x)

        # FFN block is completely bypassed
        return x # TODO # Skip FFN

class DecoderLayerNoFFN(DecoderLayer):
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:

        # Masked self-attention block
        self_attention_output = self.self_attention(x, x, x, tgt_mask)
        x = x + self.dropout1(self_attention_output)
        x = self.norm1(x)

        # Cross-attention block
        cross_attention_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        x = x + self.dropout2(cross_attention_output)
        x = self.norm2(x)
        return x # TODO

class EncoderNoFFN(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # TODO
        self.layers = nn.ModuleList([EncoderLayerNoFFN(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class DecoderNoFFN(Decoder):
     def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # TODO
        self.layers = nn.ModuleList([DecoderLayerNoFFN(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class TransformerNoFFN(TinyTransformer):
    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 256, n_heads: int = 4, n_layers: int = 2, d_ff: int = 1024, max_seq_len: int = 100, dropout: float = 0.1):
        super().__init__(src_vocab_size, tgt_vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout)

        # TODO
        self.encoder = EncoderNoFFN(n_layers, d_model, n_heads, d_ff, dropout)
        self.decoder = DecoderNoFFN(n_layers, d_model, n_heads, d_ff, dropout)

# TODO

model_No_FFN = TransformerNoFFN(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['multi_head']
).to(device) # TODO: INITIALIZE THE MODEL

# TODO -- RUN TRAINING
print(f"Multi-head Learnable Skip Weights model parameters: {count_parameters(model_No_FFN):,}")


# Training settings
LEARNING_RATE = 0.0001 # YOU CAN CHOOSE AS PER TRAINING AND VALIDATION PLOTS
N_EPOCHS = 100 # TODO

# Loss function - ignore padding token
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# ============================================
# TRAINING LOOP FOR MULTI-HEAD MODEL
# ============================================

print("\n" + "="*50)
print("TRAINING MULTI-HEAD MODEL (4 heads)")
print("="*50)

optimizer_multi = optim.Adam(model_No_FFN.parameters(), lr=LEARNING_RATE)

# Training history
train_losses_multi_No_FFN = []
val_losses_multi_No_FFN = []

best_val_loss = float('inf')

for epoch in trange(N_EPOCHS, desc="Epochs"):
    start_time = time.time()

    # Train
    train_loss = train_epoch(model_No_FFN, train_loader, optimizer_multi, criterion)

    # Evaluate
    val_loss = evaluate(model_No_FFN, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_multi_No_FFN.append(train_loss)
    val_losses_multi_No_FFN.append(val_loss)

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_no_residual.state_dict(), 'tiny_transformer__no_FFN.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
model_multi.load_state_dict(torch.load('tiny_transformer__no_FFN.pt'))

# ============================================
# TODO: PLOT TRAINING CURVES
# ============================================

plt.figure(figsize=(12, 5))

# Loss curves
plt.subplot(1, 2, 1)
plt.plot(train_losses_multi_No_FFN, label='Multi-head Train')
plt.plot(val_losses_multi_No_FFN, label='Multi-head Val')
# plt.plot(train_losses_single, label='Single-head Train')
# plt.plot(val_losses_single, label='Single-head Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# Validation loss comparison
plt.subplot(1, 2, 2)
epochs = range(1, N_EPOCHS + 1)
# plt.plot(epochs, val_losses_multi_No_FFN, 'o-', label='Multi-head')
# Plot validation loss for the baseline multi-head model (from Q1)
plt.plot(epochs, val_losses_multi, 'o-', label='Baseline (Multi-head) Validation Loss')
plt.plot(epochs, val_losses_multi_No_FFN, 's-', label='Multi-head-No-FFN')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
# plt.title('Validation Loss Comparison')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Answer the questions by training the model

"""# PART:3 (Attention Modulation)

 - Implement Token Distance as an Attention Bias (as asked in assignemnt)

## Soultion 3(A)
"""

# ==============================================================================
# TOOD: ATTENTION MODULATION (Token Distance Bias)
# ==============================================================================

class DistanceAwareMultiHeadAttention(MultiHeadAttention):
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1, max_seq_len: int = 128):
        super().__init__(d_model, n_heads, dropout)

        # We use a learnable parameter to scale the distance penalty.
        # Initialized to a small negative value to start with a penalty.
        self.distance_penalty_scaler = nn.Parameter(torch.tensor(-1.0))

        # Create a bias tensor based on the absolute distance.
        # This is not a model parameter, so we use register_buffer.
        pos = torch.arange(max_seq_len, dtype=torch.float).unsqueeze(0)
        distance_matrix = torch.abs(pos - pos.T)

        # Use a logarithmic penalty: -log(distance + 1)
        # The log dampens the penalty for very large distances.
        # The negative sign ensures it's a penalty (reduces attention score).
        log_distance_bias = -torch.log(distance_matrix + 1)
        self.register_buffer('distance_bias', log_distance_bias)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):

        seq_len = Q.size(-2)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # Add the distance bias before the softmax
        # We slice the pre-computed bias matrix to the actual sequence length of the input
        sliced_distance_bias = self.distance_bias[:seq_len, :seq_len]

        # The learnable scaler allows the model to adjust the strength of the penalty
        attn_scores = attn_scores + self.distance_penalty_scaler * sliced_distance_bias

        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(attn_scores, dim=-1)
        self.attention_weights = attention_weights  # Store for visualization

        output = torch.matmul(self.dropout(attention_weights), V)
        return output, attention_weights

class EncoderLayerDistanceAware(EncoderLayer):
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, max_seq_len: int = 128):
        super().__init__(d_model, n_heads, d_ff, dropout)
        # Override the self-attention module with our distance-aware version
        self.self_attention = DistanceAwareMultiHeadAttention(d_model, n_heads, dropout, max_seq_len)

class EncoderDistanceAware(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, max_seq_len: int = 128):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # Override the layers with our distance-aware encoder layers
        self.layers = nn.ModuleList([
            EncoderLayerDistanceAware(d_model, n_heads, d_ff, dropout, max_seq_len)
            for _ in range(n_layers)
        ])

class TransformerDistanceAware(TinyTransformer):
     def __init__(self, *args, **kwargs):
        # We need max_seq_len, so we extract it or use a default
        max_seq_len = kwargs.get('max_seq_len', 128)
        super().__init__(*args, **kwargs)

        # Extract parameters needed to initialize the new encoder
        n_layers = kwargs.get('n_layers', 2)
        d_model = kwargs.get('d_model', 128)
        n_heads = kwargs.get('n_heads', 4)
        d_ff = kwargs.get('d_ff', 256)
        dropout = kwargs.get('dropout', 0.1)

        # Only modify the encoder for this experiment for simplicity
        self.encoder = EncoderDistanceAware(n_layers, d_model, n_heads, d_ff, dropout, max_seq_len)

        # Re-initialize parameters for the new encoder
        self._init_parameters()

"""## Solution 3(B)"""

# TODO

# Train the Model
# ==============================================================================
#  TRAINING THE DISTANCE-AWARE ATTENTION MODEL
# ==============================================================================

print("="*50)
print("INITIALIZING & TRAINING DISTANCE-AWARE MODEL")
print("="*50)

# 1. Initialize the new "distance-aware" model
# Ensure max_seq_len is consistent with the value used in the attention class definition
model_distance_aware = TransformerDistanceAware(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['multi_head'],
    max_seq_len=128
).to(device)

print(f"Distance-aware model parameters: {count_parameters(model_distance_aware):,}")
# Note: The parameter count is nearly identical, with only one extra learnable scaler.

# 2. Set up optimizer and training settings
optimizer_dist_aware = optim.Adam(model_distance_aware.parameters(), lr=LEARNING_RATE)
train_losses_dist_aware = []
val_losses_dist_aware = []
best_val_loss = float('inf')

# 3. Run the training loop
for epoch in trange(N_EPOCHS, desc="Training Distance-Aware Model"):
    start_time = time.time()

    train_loss = train_epoch(model_distance_aware, train_loader, optimizer_dist_aware, criterion)
    val_loss = evaluate(model_distance_aware, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_dist_aware.append(train_loss)
    val_losses_dist_aware.append(val_loss)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_distance_aware.state_dict(), 'tiny_transformer_dist_aware.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load the best performing model for evaluation
model_distance_aware.load_state_dict(torch.load('tiny_transformer_dist_aware.pt'))

# --- Plot Final Comparison between validation loss 'Baseline (Multi-head)' and 'Distance-Aware Attention' ---
# ==============================================================================

plt.figure(figsize=(10, 6))
epochs = range(1, N_EPOCHS + 1)

# Plot validation loss for the baseline multi-head model (from Q1)
plt.plot(epochs, val_losses_multi, 'o-', label='Baseline (Multi-head) Validation Loss')

# Plot validation loss for the new distance-aware model
plt.plot(epochs, val_losses_dist_aware, 's-', label='Distance-Aware Validation Loss')

plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss Comparison: Baseline vs. Distance-Aware Attention')
plt.legend()
plt.grid(True)
plt.show()

# TODO

# ==============================================================================
#  ATTENTION VISUALIZATION FOR DISTANCE-AWARE MODEL
# ==============================================================================

print("="*50)
print("ATTENTION VISUALIZATION (DISTANCE-AWARE)")
print("="*50)

# Use the same test sentence for a direct comparison
test_sentence = "Die Katze sitzt auf dem Tisch."
print(f"Visualizing attention for: '{test_sentence}'")

# Visualize distance-aware model attention
visualize_attention(model_distance_aware, test_sentence)

print("\nAssignment complete.")